\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{fullpage}
\usepackage{enumerate}
\usepackage{graphicx}

\title{Computer Science 577 Notes \\ Machine Learning}
\author{Mendel C. Mayr}
\date{\today}

\begin{document}
	\maketitle
	\vspace{10pt}
	\begin{center}
		\includegraphics[width = 2.9in]{svm.png}
		\end{center}
	\vspace{16pt}
	\tableofcontents
	\clearpage

	\section{Decision Tree Learning}
		\subsection{Information Gain}
			Information gain is used to determine the (feature to) split \\
			Information theory: entropy and information gain:
			\begin{enumerate}[(i)]
				\item Entropy $= H(X) = -\sum_{x \in X}P(X = x)\log_2 P(X = x)$
				\item Conditional entropy $= H(Y|X) = -\sum_{x \in X}P(Y|X = x)$, where \\
				$H(Y|X = x) = \sum_{y \ in Y}P(Y = y|X = x)\log_2 P(Y = y|X = x)$
				\item Mutual information (information gain) $= I(X, Y) = H(Y) - H(Y|X)$
				\end{enumerate}
			Alternative metric: Gini coefficient, i.e. product of probabilities for each of (2) outcomes for the feature \\
			\\
			Limitation of information gain: biased toward tests with many outcomes (i.e. features with many possible values) \\
			C4.5 uses gain ratio: $SplitInfo(D, S) = -\sum_{k \in S} |D_k|/|D| \log_2(D_k/D)$, and $GainRatio = I(D, S)/SplitInfo(D, S)$
		\subsection{Decision Tree Algorithms}
			Overfitting: $h \in H$ overfits the training data $D$ if there is an alternative model $h' \in H$ such that $error(h) > error(h')$ yet $error_D(h) < error_D(h)$ \\
			Avoiding overfitting in decision tree learning:
			\begin{enumerate}[(i)]
				\item Early stopping: stop if further splitting not justified by statistical test
				\item Post-pruning: grow large tree, then prune nodes using tuning set \\
				Iteratively eliminate nodes until further reductions reduce accuracy
				\end{enumerate}
			Lookahead: instead of evaluating using information gain, look ahead to see what splits at the next level would be, and measure information gain at a deeper level \\
			\\
			Continuous features: use threshold-based boolean attribute, treshold determined by sorting examples according to the featurem and generating candidate thresholds between adjacent examples with different class values \\
			Threshold chosen from candidates based on information gain \\
			\\
			Training examples with missing attribute values: possible strategies
			\begin{enumerate}[(i)]
				\item At node $n$, upon encountering a missing feature value, assign it the value most common among examples at node $n$, or most common among examples at the node $n$ that also have the same class value
				\item Assign fractional value to attribute for the example and pass fractional example to children for purposes of evaluating information gain
				\end{enumerate}
		\clearpage

	\section{Instance-Based Learning}
		\subsection{K-nearest Neighbor}
			$k$-nearest neighbor classification: given an instance $x_q$ to classify, find $k$ training-set instances that are most simimlar or $x_q$ \\
			Return the class value: $\hat{y} = argmax_{v \in values(Y)} \sum_{i = 1}^k \delta(v, y_i)$ \\
			Various determinations of distance: 
			\begin{enumerate}[(i)]
				\item Hamming distance: number of features with differing values
				\item Euclidean distance: $\delta(x_i, x_j) = \sqrt{\sum_f ({x_i}_f - {x_j}_f)^2}$
				\item Manhattan distance: $\delta(x_i, x_j) = \sum_f ({x_i}_f - {x_j}_f)^2$
				\end{enumerate}
			$k$-nearest neighbor regression: given an instance $x_q$, find the $k$ nearest training-set instances and return $\sum_{i = 1}^k \delta(v, y_i)$ \\
			Distance-weighted nearest neighbor: instances contribute to prediction according to their distance from $x_q$ \\
			\\
			$k$-nearest neighbor does almost nothing at training time, and offsets costs to classification/prediction time \\
			Strategies to speed up $k$-nearest neighbor
			\begin{enumerate}[(i)]
				\item Don't retain every training instance: edited nearest neighbor \\
				Select subset of instances that still provide accurate classifications: \\
				\begin{enumerate}[(a)]
					\item Incremental deletion: delete from memory all training instances redundant to classification
					\item Incremental growth: if training instances insufficient to classify training instance, add to memory
					\end{enumerate}
				\item Use data structure to look up nearest neighbors ($k$-$d$ tree)
				\end{enumerate}
			$k$-$d$ trees ($A^*$ instance search): each node stores one instance, and splits on the median value of the feature having the highest variance \\
			Nodes are pushed to the $A^*$ priority queue with the value indicating the minimum possible distance to the query based on the threshold for the split \\
			\\
			Strenghts of instance based learning: simple, efficeint training, easily adapts to on-line nearning, rubust to noisy data, etc. \\
			Limitations: sensitive to range of feature values, senstivie to irrelevant and correlated features, inefficient classification, no insight into problem domain (i.e. lacks modeling of problem)
		\subsection{Linear Locally Weighted Regression}
			Locally weighted linear regression: $f(x) = w_0 + w_1a_1(x) +\:...\:+ w_na_n(x)$ \\
			Local approximations to have query point fit local training examples:
			\begin{enumerate}
				\item Minimize squared error over just $k$ nearest neighbors: \\
				$E_1(x_q) = (1/2)\sum_{x \in k\text{ nearest neighbors of }x_q} (f(x) - \hat{f}(x))^2$
				\item Minimize squared error over entire set of $D$ of training examples, using decreasing function $K$: \\
				$E_2(x_q) = (1/2)\sum_{x \in D} (f(x) - \hat{f}(x))^2 K(d(x_q, x))$
				\item Combination of the previous: let $N$ be $k$ nearest neighbors of $x_q$
				\begin{equation*}
					\frac{1}{2}\sum_{x \in N} (f(x) - \hat{f}(x))^2 K(d(x_q, x))
					\end{equation*}
				\end{enumerate}
		\clearpage

	\section{Probability and Bayesian Learning}
		Note: for more rigourous details, see Heckerman's Bayesian Network Learning Tutorial
		\subsection{Probabilistic Machine Learning Concepts}
			Recall Bayes theorem: $P(A|B) = P(B|A)P(A)/P(B)$ \\
			\\
			Brute-force MAP learning algorithm:
			\begin{enumerate}[(i)]
				\item Given information $D$, for each hypothesis $h \in H$, calculate the posterior probability $P(h|D) = P(D|h)P(h)/P(D)$
				\item Output hypothesis $h_{MAP}$ with highest posterior probability $h_{MAP} = argmax_{h \in H} P(h|D)$
				\end{enumerate}
			Other probabalisitic concepts in machine learning:
			\begin{enumerate}[(i)]
				\item Maximum likelihood and least-squared error hypotheses
				\item Maximum likelihood hypotheses for predicting probabilities
				\item Minimum description length principle
				\item Bayes optimal classification: $argmax_{v_j \in V}\sum_{h_i \in H} P(v_j|h_i)P(h_i|D)$
				\item Gibb's Algorithm: choose hypothesis $h \in H$ at random, according to posterior probaiblity distribution over $H$, and use $h$ to predict the classification of the next instance $x$ 
				\end{enumerate}
		\subsection{Bayesian networks}
			A Bayesian network consists of a directed acyclic graph and a set of conditional probability distributions \\
			In the each Directed Acyclic Graph (DAG):
			\begin{enumerate}[(i)]
				\item Each node denotes a random variable
				\item An edge from $X$ to $Y$ represents that $X$ directly influences $Y$
				\item Each node $X$ contains a conditional probability distribution representing $P(X|Parents(X))$
				\item Each variable $X$ is independent of its non-descendants given its parents
				\item Each variable $X$ is independent of all others given its Markov blanket
				\end{enumerate}
			Advantages of Bayesian network representation:
			\begin{enumerate}[(i)]
				\item Captures indepdendence and conditional indpendence where they exist
				\item Encodes the relevant  portion fo the full joint distribution
				\item Graphical representation gives insight into complexity of inference
				\end{enumerate}
			Inference task: given values for some variables in the network (evidence) and set of query variables, compute posterior distribution over query variables \\
			Hidden variables: neither evidence nor the query variables \\
			Baysean networks allow for any set to be evidence and any set to be query \\
			Inference by enumeration: consider the chain rule $P(x_1, ..., x_n) = P(x_1)P(x_2|x_1)P(x_3|x_2, x_1)...P(x_n|x_{n-1}, ..., x_1)$ \\
			Posterior probability on query variables can be found via independence, chain rule, and marginalization \\
			\\
			Parameter learning task: given set of training instances and graph structure, infer parameter of conditional probability distributions \\
			Strcture learning task: given set of training instances, infer graph structure (and possibly parameters of CPDs) \\
			For parameter learning, use maximum a posteriori (MAP) estimation: e.g. m-estimates $P(X = x) = (n_x + p_xm)/(\sum_{v \in Values(X)} n_v) + m$, where $p_x$ is prior probability of value $x$, $m$ is number of virtual instances, and $n_v$ is number of occurences of value $v$ 
		\subsection{Expectation Maximization}
			Missing data (hidden variables, values missing at random): values can be imputed using Expectation Maximization (EM) \\
			Iterate until convergence:
			\begin{enumerate}[(i)]
				\item Expectation step: using current model, compute expectation over missing values (missing temporarily take expected values)
				\item Maximization step: update model parameters with those that maximize probability of data (MLE or MAP)
				\end{enumerate}
			Note that $k$-means unsupervised clustering is a form of expectation maximization \\
			Expectation maximation can be hard (takes most likely value) or soft (expectation is probabiltiy distribution) \\
			\\
			Expectation maximation for parameter learning:
			\begin{enumerate}[(i)]
				\item Expecation step: compute probability of each completion of incomplete data points, i.e. answering query over missing variables given others
				\item Maximization step: use completed data set to update Dirchlet distributions, except counts can be fractional, update conditional probability tables
				\end{enumerate}
			Subtelty for parameter learning: overcounting based on number of iterations required to converge to settings for missing values. After each expectation step, reset Dirichlet distributions before repeating maximization step \\
			\\
			Problems with expectation maximization: only finds local optimum, deterministic with respect to priors
		\subsection{Learning Network Structure}
			Chow-Liu algorithm: learns tree structure that maximizes likelihood of training data
			\begin{enumerate}[(i)]
				\item Compute weight $I(X_i, X_j)$ of each possible edge $(X_i, X_j)$
				\item Find maximum-weight spanning tree: use mutual information to calculate \\
				$I(X, Y) = \sum_{x \in values{X}}\sum_{y \in values{Y}} P(x, y)\log_2 P(x, y)/(P(X)P(Y))$ \\
				\\
				Prim's algorithm (given $(V, E)$):
				\begin{enumerate}[(a)]
					\item $V_{new} = \{v\}$ where $v \in V$ (arbitrary)
					\item $E_{new} = \{\}$
					\item Repeat until $V_{new} = V$: choose edge $(u, v)$ in $E$ with max weight where $u$ is in $V_{new}$ and $v$ is not. Add $v$ to $V_{new}$ and $(u, v)$ to $E_{new}$
					\item Return $(V_{new}, E_{new}$, a maximum spanning tree
					\end{enumerate}
				Kruskal's algorithm (given $(V, E)$):
				\begin{enumerate}[(i)]
					\item $E_{new} = \{\}$
					\item For each $(u, v)$ in $E$ ordered by weight (high to low): \\
					Pop $(u, v)$ from $E$ and add to $E_{new}$ if it does not create a cycle
					\item Return $V$ and $E_{new}$, a maximum spanning tree
					\end{enumerate}
				\item Assign edge directions in maximimum-weight spanning tree \\
				Pick a node for the root, assign edge direction
				\end{enumerate}
			Heuristic search for structure learning: each state in search space represents Bayes net structure \\
			Search approach requires specification of
			\begin{enumerate}
				\item Scoring function: $score(G, D) = \sum_i score(X_i, Parents(X_i):D)$ \\
				Thus, a network can be scored by summing terms over nodes, and changes can be efficently scored via a local search
				\item State transition operators: adding an edge, deleting in edge, reversing an edge
				\item Search algorithm: hill climbing or sparse candidate search
				\end{enumerate}
			Bayesian network hill climibing search: greedy algorithm \\
			Bayesian network sparse candidate search (given data set $D$, initial network $B_0$, parameter $k$):
			\begin{enumerate}[(i)]
				\item Let $i = 0$
				\item Repeat until convergence:
				\begin{enumerate}[(a)]
					\item Increment $i$
					\item Restrict step: select for each variable $X_j$ a set $C_j^i(|C_k^i|)$ of candidate parents
					\item Maximize step: find network $B_i$ maximizing score among networks where $\forall X_j, Parents(X_j) \subset C_j^i$
					\end{enumerate}
				\item Return $B_i$
				\end{enumerate}
			Restriction step in sparse candidate search: \\
			Fpr the first iteration: candidate parents computed using mutual information \\
			$I(X, Y) = \sum_{x, y} P(x, y)\log{(P(x, y)/(P(x)P(y)))}$ \\
			Kullback-Liebler divergence: distance measure between two distributions $P$ and $Q$
			\begin{equation*}
				D_{KL} = (P(X)||Q(X)) = \sum\limits_x P(x)\log\frac{P(x)}{Q(x)}
				\end{equation*}
			KL-divergence assesses discrepancy between network's estimate $P_{net}(X, Y)$ and empirical estimate \\
			$M(X, Y) = D_{KL}(P(X, Y)||P_{net}(X, Y))$ \\
			\\
			Algorithm for restriction step in sparse candidate (given data $D$, current network $B_i$, parameter $k$):
			\begin{enumerate}[(i)]
				\item For each variable $X_j$
				\begin{enumerate}[(a)]
					\item Calculate $M(X_j, X_i)$ for all $X_j \neq X_i$ such that $X_l \notin Parents(X_j)$
					\item Choose highest ranking $X_l...X_{k - s}$ where $s = |Parents(X_j)|$
					\item Include current parents in candidate set to ensure monotonic score improvements \\
					$C_j^i = Parents(X_j) \cup X_l ... X_{k - s}$
					\end{enumerate}
				\item Return ${C_j^i}$ for all $X_j$
				\end{enumerate}
			Scoring function for structure learning: maxmize data probability, but penalize complexity \\
			General approach: $argmax_{G, \theta_G} \log{P(D|G, \theta_G)} - f(n)|\theta_G|$ \\
			Akaike information criterion (AIC) $f(n) = 1$, Bayesian Information Criterion (BIC) $f(n) = \log{(n)}/2$
		\subsection{Naive Bayes and Tree Augmented Network}
			Naive Bayes assumption: all features $X_i$ are conditionally independent given class $Y$ \\
			$P(X_1, ..., X_n, Y) = P(Y)\Pi_{i = 1}^nP(X_i|Y)$ \\
			Unaugmented tree starts with edge from node $Y$ to each feature $X_1, ..., X_n$
			\\
			Learning: estimate $P(Y = y)$ for each value of $Y$, estimate $P(X_i = x|Y = y)$ for each $X_i$ \\
			Classification done using Bayes rule: 
			\begin{equation*}
				P(Y = y|X) = \frac{P(y)P(X|y)}{\sum\limits_{y' \in values(Y)}P(y')P(X|y')} = \frac{P(y)\prod\limits_{i = 1}^n P(x_i|y)}{\sum\limits_{y' = values(Y)}(P(y')\prod\limits_{i = 1}^n P(x_i|y'))}
				\end{equation*}
			Tree Augmented Network (TAN) algorithm: learns tree structure to augment edges of naive Bayes network
			\begin{enumerate}[(i)]
				\item Compute weight $I(X_i, X_j|Y)$ for each possible edge $(X_i, X_j)$ between features
				\item Find maximum weight spanning tree (MST) for graph over $X_l ... X_n$
				\item Assign edge direction in MST
				\item Construct a TAN model by adding node for $Y$ and an edge for $Y$ to each $X_i$
				\end{enumerate}
		\clearpage

	\section{Machine Learning Methdology}
		\subsection{Partitioning of Data}
			Evaluation of learning models: given instance set $X$ and probability distribution $D$ defining the probaiblity of encountering an $x \in X$, learner must learn target concept (function) $f \in H$ \\
			Evaluation methdology seeks to answer two questions: what is the best accuracy for a hypothesis $h$, learned from $n$ instances, applied to future instances drawn according to $D$, and what is the probably error in this accuracy estimate \\
			\\
			Distinction between sample error and true error:
			\begin{enumerate}[(i)]
				\item Sample error: error of $h$ with respect ot target function $f$ and data sample $S$ \\	
				$error_S(h) = (1/n)\sum_{x \in S} \delta(f(x), h(x))$, where $\delta(a, b) = 1$ if $a \neq b$, 0 otherwise
				\item True error: $error_D(h) = Pr_{x \in D} [f(x) \neq h(x)]$
				\end{enumerate}
			Limitations of singular training/test partitions: 
			\begin{enumerate}[(i)]
				\item Not enough data for sufficiently large training and test sets 
				\item Single training set does not show how sensitive accuracy is to particular training sample
				\end{enumerate}
			Random resampling: repeatedly partitioning data into training and test sets \\
			Stratified sampling: statify instances by class and select (i.e. maintain class proportions) \\
			Cross validation: partition data into $n$ subsamples. Iteratively test on one subset, train on rest \\
			Leave-one-out cross validation: $n$ is the number of instances 
		\subsection{Performance Evaluation}
			Receiver Operating Characteristic (ROC) curve: true positive-rate vs false positive-rate as threshold as confidence of an instance being positive is varied \\
			Algorithm for creating an ROC curve: 
			\begin{enumerate}[(i)]
				\item Sort test-set prediction according to confidence that each instance is positive
				\item Step through sorted list from high to low confidence
				\begin{enumerate}[(a)]
					\item Locate threshold between instances with opposite classes
					\item Compute TPR, FPR for instances above threshold
					\item Output (FPR, TPR) coordinate
					\end{enumerate}
				\end{enumerate}
			Points plotted on ROC curve can be interpolated to form convex hull \\
			Having a high TPR or low FPR does not indicate accuracy in unbalanced data sets \\
			\\
			Alternative accuracy metric: recall and precision
			\begin{enumerate}[(i)]
			 	\item Recall (TP rate) = TP/(actual positives) = TP/(TP + FN)
			 	\item Precision = TP/(predicted positives) = TP/(TP + FP)
				\end{enumerate}
			Precision/recall curve: plots prevision vs. recall as threshold as confidence of an instance being positive is varied \\
			\\
			Single ROC/PR curve with cross-validation: multiple approaches
			\begin{enumerate}[(i)]
				\item Assume confidence values comparable across folds \\
				Pool predicitons from all test sets and plot curve from pooled predictions
				\item Plot individual curves for all test sets, viewing each as function \\
				Plot average curve for set of functions 
				\end{enumerate}
			ROC and PR curves allow predictive performance to be assessed at various levels of confidence, assume binary classification, and can be summarized by the integral \\
			ROC curves: insensitive to class distribution changes, can identify optimal classification thresholds for tasks with differential misclassification costs \\
			PR curves: show fraction of predictions that are false positives, suited for tasks with many negative instances
		\subsection{Confidence Intervals and Learning Tasks}
			Avoiding pitfalls: questions when applying learning tasks
			\begin{enumerate}[(i)]
				\item Held-aside test data should be representative of collecting new data
				\item Folds of cross validation should only use training data for the fold \\
				At no point in preprocessing should test case labels be accessed
				\item Repeated modificiations of learning algorithm or preprocessing could lead to overfitting
				\end{enumerate}
			Confidence intervals on error: suppose we have learned model $h$, test set $S$ containing $n$ instances drawn independently of each other and independent of $h$, with $n \geq 30$, and $h$ makes $r$ errors over the $n$ instances \\
			Estimate of error is $error_S(h) = r/n$ \\
			With approximately $N$ probability, the true error is in the interval
			\begin{equation*}
				error_S(h) \pm z_N \sqrt{\frac{error_S(h)(1 - error_S(h))}{n}}
				\end{equation*}
			where $z_N$ is a constant that depends on $N$, e.g. $z_{0.95} = 1.96$ \\
			Confidence interval follows from normal approximation of binomial distribution \\
			An $N\%$ confidence interval on parameter $p$ is an interal that is expected with $N\%$ probability to contain $p$ \\
			In using confidence intervals, consider (central limit theorem) that a large number independent, identically distribution random variables approximately follows a normal distribution \\
			\\
			Difference in error of two hypotheses: $\hat{d} = error_S(h_1) - error_S(h_2)$ \\
			$N\%$ confidence interval for $d$ is:
			\begin{equation*}
			 	\hat{d} \pm z_N \sqrt{\frac{error_{S_1}(h_1)(1 - error_{S_1}(h_1))}{n_1} + \frac{error_{S_2}(h_2)(1 - error_{S_2}(h_2))}{n_2}}
				\end{equation*}
		\subsection{Comparing Learning Models and Hypotheses}
			Consider $\delta$s as observed values of a set of independent, identically distributed variables \\
			Null hypothesis: the 2 learning systems have the same accuracy \\
			Alternative hypothesis: one system is more accurate than the other \\
			Hypothesis test:
			\begin{enumerate}[(i)]
				\item Use paired $t$-test to determine probability $p$ that mean of $\delta$s would arise from null hypothesis
				\item If $p$ is sufficiently small (usually $p < 0.05$), reject null hypothesis
				\end{enumerate}
			Comparing systems using a paired $t$-test: 
			\begin{enumerate}[(i)]
				\item Calculate sample mean: $\bar{\delta} = (1/n)\sum_{i = 1}^n \delta_i$
				\item Calculate $t$ statistic:
				\begin{equation*}
					t = \frac{\bar{\delta}}{\sqrt{\frac{1}{n(n - 1)}\sum\limits_{i = 1}^n (\delta_i - \bar{\delta})^2}}
					\end{equation*}
				\item Determine corresponding $p$-value: use $t$ in table for $t$-distribution with $n - l$ degrees of freedom
				\end{enumerate}
			$t$-tests compare two learning systems, tests like McNemar's $\chi^2$ test compares two learned models \\
			\\
			Scatter plots for pairwise method comparison: can compare thwo methods $A$ and $B$ by plotting $(A performance, B performance)$ across numerous data sets \\
			Lesion studies: determine relevant contributions to learning system performance by removing (lesioning) components
		\clearpage

	\section{Computational Learning Theory}
		\subsection{PAC Learning}
			Computational learning theory concerns itself with: 
			\begin{enumerate}[(i)]
				\item Sample complexity: how many training examples needed to converge on hypothesis
				\item Computational complexity: how much computational effort needed for learner to converge
				\item Mistake bound: how many misclassifications before learner converges on hypothesis
				\end{enumerate}
			Learning setting composed of:
			\begin{enumerate}[(i)]
				\item Set of instances $X$
				\item Set of hypotheses $H$
				\item Set of possible target concepts $C$
				\item Unknown probability distribution $D$ over instances
				\end{enumerate}
			Learner is given set $D$ of training instances $(x, c(x))$ for some target concept $c \in C$ \\
			Learning task is to output hypothesis $h$ modeling $c$ \\
			\\
			True error of hypothesis: how often $h$ is wrong on future instances drawn from $D$: \\
			$error_D(h) = P_{x \in D}(c(x) \neq h(x))$ \\
			Traning error: how often $h$ is wrong on instances in training set $d$ \\
			$error_D(h) = P(x \in D)(c(x) \neq h(x)) = (1/|D|)\sum_{x \in D}\delta(c(x) \neq h(x))$ \\
			\\
			Probably Approximately Correct (PAC) Learning: consider class $C$ of possible target concepts defined over set of instances $X$ of size $n$, and a learner $L$ using hypothesis space $H$ \\
			$C$ is PAC learnable by $L$ if for all: $c \in C$, distributions $D$ over $C$, $\varepsilon$ and $\delta$ such that $0 < \varepsilon, \delta < 0.5$ \\
			$L$ will, with probability $> (1 - \delta)$, output hypothesis $h \in H$ such that $error_D(h) \leq \varepsilon$, in time polynomial to $1/\varepsilon, 1/\delta, n, size(c)$ \\
			Suppose hypotheses consistent with $m$ training instances \\
			PAC learnability determined by whether
			\begin{enumerate}[(i)]
				\item $m$ grows polynomially in the relevant parameters
				\item Processing time per training exmaple is polynomial
				\end{enumerate}
			Consistency with respect to training example set $D = \{(x_1, c(x_1), ..., (x_m, c(x_m)))\}$, and hypothesis $h$
			\begin{equation*}
				consistent(h, D) = \forall((x, c(x)) \in D) h(x) = c(x)
				\end{equation*}
			Version space: $VS_{H, D} = \{h \in H | consistent(h, D)\}$ \\
			Version space $VS_{H, D}$ is $\varepsilon$-exhausted with repsect to concept $c$ and data set $D$ if: \\
			$(\forall h \in VS_{H, D}) error_D(h) < \varepsilon$ \\
			\\
			The probability that $VS_{HD}$ is not $\varepsilon$-exhausted is no greater than $|H|e^{-\varepsilon m}$ \\
			Proof: probability that some hypothesis with error greater than $\varepsilon$ is consistent with $m$ training instance is $(1 - \varepsilon)^m$, there are at most $|H|$ such hypotheses, and since $(1 - \varepsilon) \geq e^{-\varepsilon}$ when $0 \leq \varepsilon \leq 1$, the bound is $|H|e^{-\varepsilon m}$ \\
			\\
			The probability must be reduced below $\delta$: $|H|e^{-\varepsilon m} \geq \delta$ \\
			Solving for $m$ yields the number of examples needed for PAC learnability:
			\begin{equation*}
				m \geq \frac{1}{\varepsilon}\left(\ln{|H|} + \ln{\frac{1}{\delta}}\right)
				\end{equation*}
		\subsection{Hypothesis spaces}
			Agnostic learning setting: don't assume $c \in H$, learner returns hypothesis $h$ that makes fewest errors in training \\
			For error on training set to be less than $error_D(h) + \varepsilon$, $m$ examples needed, where
			\begin{equation*}
				m \geq \frac{1}{2\varepsilon^2}\left(\ln{|H|} + \ln{\frac{1}{\delta}}\right)
				\end{equation*}
			If $H$ is infinite, for measure of hypothesis space complexity, use in place of $|H|$ the largest subset of $X$ for which $H$ can guarantee zero training error regardless of target function \\
			\\
			A set of instances $D$ is shattered by a hypothesis space $H$ if and only if for every dichotomy of $D$, there is a hypothesis in $H$ consistent with the dichotomy \\
			The VC dimension of $H$ defined over instance space $X$ is the size of the largest finite subset of $X$ shattered by $H$ \\
			The VS dimension for finite $H$: $VC-dim(H) \leq \log_2|H|$ \\
			Proof: suppose $VC-dim(H) = d$, and for $d$ instances, there are $2^d$ different possible labelings. $H$ must represent $2^d$ hypotheses, so $2^d \leq |H|$, thus $d \leq \log_2|H|$ \\
			Using $VC-dim(H)$ as a measure of complexity of $H$, the bound can be defined as
			\begin{equation*}
				m \geq \frac{1}{\varepsilon}\left(4\log_2\frac{2}{\delta} + 8VC-dim(H)\log_2\frac{13}{\varepsilon}\right)
				\end{equation*}
			Lower bound on sample complexity: given target concept $C$
			\begin{equation*}
				m < max\left[\frac{1}{\varepsilon}\log{|\frac{1}{\delta}|},  \frac{VC-dim(C) - 1}{32\varepsilon}\right]
				\end{equation*}
		\subsection{Mistake Bound}
			Learning setting (i.e. on-line learning setting): for $t = 1, 2, ...$
			\begin{enumerate}[(i)]
				\item Learner receives instance $x_t$
				\item Learner predicts $h(x_t)$
				\item Learner receives label $c(x_t)$ and updates model $h$
				\end{enumerate}
			Mistake bound model addresses how many mistakes will be made before the target concept is learned \\
			\\
			Mistake bound model analysis of halving algorithm \\
			Halving algorithm is as follows:
			\begin{enumerate}[(i)]
				\item $VS_1 = H$
				\item For $t = 1$ to $T$:
				\begin{enumerate}[(a)]
					\item Given training instance $(x_t, c(x_t))$, $h'(x_t) = MajorityVote(VS_t, x_t)$
					\item Eliminate all wrong $h$ from version space (at least half) \\
					$VS_{t + 1} = \{h \in VS_t | h(x_t) = c(x_t)\}$
					\end{enumerate}
				\item Return $VS_{t + 1}$
				\end{enumerate}
			Maximum number of mistakes is $M_{Halving}(C) = \lfloor log_2|H|\rfloor$ \\
			\\
			Optimal mistake bounds: suppose $C$ is an arbitrary nonempty concept class \\
			An optimatal mistake bound is defined as $Opt(C) = min_{A \in \text{Learning Algorithms}} M_A(C)$ \\
			$Opt(C)$ is the number of mistakes made on the hardest target concept and hardest training sequence by best algorithm \\
			Optimal mistake bound is $VC-dim(C) \leq Opt(C) \leq M_{Halving}(C) \leq log_2(|C|)$ \\
			\\
			Mistake bound model analysis of weighted majority algorithm \\
			Weighted majority algorithm is as follows: given predictors $A = \{a_1, ..., a_n\}$, learning rate $0 \leq \beta < 1$
			\begin{enumerate}[(i)]
				\item For all $i$, initialize $w_i = 1$
				\item For each training instance $(x, c(x))$:
				\begin{enumerate}[(a)]
					\item Initialize $q_0$ and $q_0$ to 0
					\item For each predictor $a_i$: $q_{a_i(x)} = q_{a_i(x)} + w_i$
					\item If $q_1 > q_0$ then $h(x) = 1$, else if $q_0 > q_1$ then $h(x) = 0$, else $h(x) = random(0, 1)$
					\item For each predictor $a_i$: if $a_i(x) \neq c(x)$ then $w_i = \beta w_i$
					\end{enumerate}
				\end{enumerate}
			Weighted majority similar to perceptron (linear separator, robust against noise, mutliplicative weight updates) \\
			If $k$ is minimum number of mistakes made by the best predictor in $A$ on training set $D$, then maximum number of mistakes for $\beta = 1/2$ is $2.4(k + log_2 n)$
		\clearpage

	\section{Ensemble Methods}
		\clearpage

	\section{Neural Networks and Deep Learning}
		\clearpage

	\section{Support Vector Machines}
		\clearpage

	\section{Reinforcement Learning}
		\clearpage
	
	\section{Rule Learning and Inductive Logic Programming}
		\clearpage	

	\section{Statistical Relational Learning}
		\clearpage

	\section{Bias-Variance Tradeoff}
		\clearpage

	\section{Real-Valued Prediciton Methods}
		\clearpage

	\section{Dimensionality Reduction}
		\clearpage

	\appendix


	\end{document}